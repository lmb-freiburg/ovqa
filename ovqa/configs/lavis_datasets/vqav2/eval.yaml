# Copyright (c) 2022, salesforce.com, inc.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause

datasets:
    vqav2:
        # data_dir: ${env.data_dir}/datasets
        data_type: images # [images|videos|features]

        build_info:
            # Be careful not to append minus sign (-) before split to avoid itemizing
            annotations:
                val:
                    url:
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_val_eval.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/v2_OpenEnded_mscoco_val2014_questions.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/v2_mscoco_val2014_annotations.json
                    storage:
                        - dataset_cache/vqav2/annotations/vqa_val_eval.json
                        - dataset_cache/vqav2/annotations/answer_list.json
                        - dataset_cache/vqav2/annotations/v2_OpenEnded_mscoco_val2014_questions.json
                        - dataset_cache/vqav2/annotations/v2_mscoco_val2014_annotations.json
                minival:  # ~25k holdout set to enable training on the val dataset
                    url:
                        - ovqa/annotations/vqav2/minival_meta.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json
                        - ovqa/annotations/vqav2/minival_questions.json
                        - ovqa/annotations/vqav2/minival_annotations.json
                    storage:
                        - ovqa/annotations/vqav2/minival_meta.json
                        - dataset_cache/vqav2/annotations/answer_list.json
                        - ovqa/annotations/vqav2/minival_questions.json
                        - ovqa/annotations/vqav2/minival_annotations.json
                nominival:  # val data to train on, ~190k samples, all val except minival
                    url:
                        - ovqa/annotations/vqav2/nominival_meta.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json
                        - ovqa/annotations/vqav2/nominival_questions.json
                        - ovqa/annotations/vqav2/nominival_annotations.json
                    storage:
                        - ovqa/annotations/vqav2/nominival_meta.json
                        - dataset_cache/vqav2/annotations/answer_list.json
                        - ovqa/annotations/vqav2/nominival_questions.json
                        - ovqa/annotations/vqav2/nominival_annotations.json
                smallval:  # very small 600 debugging set
                    url:
                        - ovqa/annotations/vqav2/smallval_meta.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json
                        - ovqa/annotations/vqav2/smallval_questions.json
                        - ovqa/annotations/vqav2/smallval_annotations.json
                    storage:
                        - ovqa/annotations/vqav2/smallval_meta.json
                        - dataset_cache/vqav2/annotations/answer_list.json
                        - ovqa/annotations/vqav2/smallval_questions.json
                        - ovqa/annotations/vqav2/smallval_annotations.json
                test:
                    url:
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/vqa_test.json
                        - https://storage.googleapis.com/sfr-vision-language-research/LAVIS/datasets/vqav2/answer_list.json
                    storage:
                        - dataset_cache/vqav2/annotations/vqa_test.json
                        - dataset_cache/vqav2/annotations/answer_list.json
            images:
                storage: ${oc.env:ENV_DATA_DIR}/coco/images
